{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crawler类（爬虫类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from pysqlite2 import dbapi2 as sqlite\n",
    "from bs4 import BeautifulSoup,Tag,CData\n",
    "from urlparse import urljoin\n",
    "\n",
    "class crawler:\n",
    "    #初始化crawler类并传入数据库名称：\n",
    "    def __init__(self,dbname):\n",
    "        self.con = sqlite.connect(dbname)\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.con.close()\n",
    "    \n",
    "    def dbcommit(self):\n",
    "        self.con.commit()\n",
    "        \n",
    "    \n",
    "    #辅助函数，用于获取条目的ID，并且如果条目不存在，就将其加入数据库：\n",
    "    def getentryid(self,table,field,value,createnew = True):\n",
    "        cur = self.con.execute(\"select rowid from %s where %s='%S'\" % (table,field,value))\n",
    "        res = cur.fetchone()\n",
    "        if res == None:\n",
    "            cur = self.con.execute(\"insert into %s (%s) values ('%s') \" % (table,field,value))\n",
    "            return cur.lastrowid\n",
    "        else:\n",
    "            return res[0]\n",
    "    \n",
    "    #为每个网页建立索引\n",
    "    def addtoindex(self,url,soup):\n",
    "        if self.isindexed(url):return\n",
    "        print 'Indexing %s'% url\n",
    "        \n",
    "        #获取每个单词\n",
    "        text = self.gettextonly(soup)\n",
    "        words = self.separatewords(text)\n",
    "        \n",
    "        #得到URL的id\n",
    "        urlid = self.getentryid('urllist','url',url)\n",
    "        \n",
    "        #将每个单词与该url关联\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in ignorewords:continue\n",
    "            self.con.execute(\"insert into wordlocation(urlid,wordid,location) \\ values (%d %d %d)\" % (url,wordid,i))\n",
    "        \n",
    "    #从一个HTML网页中提取文字\n",
    "    def gettextonly(self,soup):\n",
    "        #string 获取标签里面的内容\n",
    "        v = soup.string\n",
    "        if v == None:\n",
    "            # 可以将tag的节点以列表的方式输出\n",
    "            c = soup.contents\n",
    "            resulttext = ''\n",
    "            for t in c :\n",
    "                subtext = self.gettextonly(t)\n",
    "                resulttext += subtext + '\\n'\n",
    "            return resulttext\n",
    "        else:\n",
    "            return v.strip()\n",
    "        \n",
    "    \n",
    "    #如果URL已经建过索引，则返回true\n",
    "    def isindexed(self,url):\n",
    "        u = self.con.execute(\"select rowid from urllist where url = '%s'\" % url).fetchone()\n",
    "        if u != None:\n",
    "            #检查它是否已经被检索过了\n",
    "            v = self.con.execute('selet * from wordlocation where urlid = %d' % u[0]).fetchone()\n",
    "            if v != None:return True\n",
    "        return False\n",
    "       \n",
    "    #添加一个关联两个网页的链接\n",
    "    def addlinkref(self,urlFrom,urlTo,linkText):\n",
    "        pass\n",
    "    \n",
    "    #根据任何非空白字符进行分词\n",
    "    def separatewords(self,text):\n",
    "        splitter = re.compile('\\\\W*')\n",
    "        return [s.lower() for s in splitter.split(text) if s != '']\n",
    "    \n",
    "    #从一小组网页开始进行广度优先搜索，直至某一给定深度\n",
    "    #期间为网页建立索引\n",
    "    def crawl(self,pages,depth = 3):\n",
    "        for i in range(depth):\n",
    "            newpages = set()\n",
    "            for page in pages:\n",
    "                try :\n",
    "                    c = urllib2.urlopen(page)\n",
    "                except:\n",
    "                    print \"Could not open %s\" % page\n",
    "                    continue\n",
    "                soup = BeautifulSoup(c.read(),\"html5lib\")\n",
    "                self.addtoindex(page,soup)\n",
    "                \n",
    "                links = soup('a')\n",
    "                for link in links:\n",
    "                    if ('href' in dict(link.attrs)):\n",
    "                        url = urljoin(page,link['href'])\n",
    "                        if url.find(\"'\")!=-1:continue\n",
    "                        url = url.split('#')[0] #去掉位置部分\n",
    "                        if url[0:4] == 'http:'and not self.isindexed(url):\n",
    "                            newspages.add(url)\n",
    "                        linkText = self.gettextonly(url)\n",
    "                        self.addlinkref(page,url,linkText)\n",
    "                        \n",
    "                self.dbcommit()\n",
    "            pages = newpages\n",
    "  \n",
    "    def createindextables(self):\n",
    "        self.con.execute('create table urllist(url)')\n",
    "        self.con.execute('create table wordlist(word)')\n",
    "        self.con.execute('create table wordlocation(urlid,wordid,location)')\n",
    "        self.con.execute('create table link(fromid integer,toid integer)')\n",
    "        self.con.execute('create table linkwords(wordid,linkid)')\n",
    "        self.con.execute('create index wordidx on wordlist(word)')\n",
    "        self.con.execute('create index urlidx on urllist(url)')\n",
    "        self.con.execute('create index wordurlidx on wordlocation(wordid)')\n",
    "        self.con.execute('create index urltoidx on link(toid)')\n",
    "        self.con.execute('create index urlfromidx on link(fromid)')\n",
    "        self.dbcommit()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mainFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: urllist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-e6bb96609cbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpagelist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'http://www.downloadsquad.com/rss.xml'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcrawler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'searchindex.db'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpagelist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-1d72449fc187>\u001b[0m in \u001b[0;36mcrawl\u001b[1;34m(self, pages, depth)\u001b[0m\n\u001b[0;32m     90\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"html5lib\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddtoindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-1d72449fc187>\u001b[0m in \u001b[0;36maddtoindex\u001b[1;34m(self, url, soup)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m#为每个网页建立索引\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maddtoindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misindexed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m'Indexing %s'\u001b[0m\u001b[1;33m%\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-1d72449fc187>\u001b[0m in \u001b[0;36misindexed\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m#如果URL已经建过索引，则返回true\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0misindexed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"select rowid from urllist where url = '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mu\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;31m#检查它是否已经被检索过了\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: urllist"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup,Tag,CData\n",
    "from urlparse import urljoin\n",
    "\n",
    "#构造一个单词列表，这些单词将被忽略\n",
    "ignorewords = set(['the','of','to','and','a','in','is','it'])\n",
    "pagelist = ['http://www.downloadsquad.com/rss.xml']\n",
    "crawler = crawler('searchindex.db')\n",
    "crawler.crawl(pagelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"\" href=\"/\">\\n\\t\\t\\t<h1>\\n\\t\\t\\t  <svg aria-label=\"Engadget\" class=\"th-title header-logo\" role=\"img\">\\n\\t\\t\\t    <title>Engadget</title>\\n\\t\\t\\t    <use xlink:href=\"#icon-logo\"></use>\\n\\t\\t\\t  </svg>\\n\\t\\t\\t</h1>\\n\\t\\t\\t</a>,\n",
       " <a class=\"increase-hit th-meta\" data-behavior=\"Pop\" data-engadget-popname=\"engadget_auth_window\" href=\"https://www.engadget.com/user/login\" target=\"_blank\">            <span class=\"increase-hit__inner\">\\n                            Login\\n                          </span>\\n            </a>,\n",
       " <a class=\"hide@tl+ table-cell ta-c c-gray-5 w-60@s w-80@m+ full-height border-left\" data-search-box-trigger=\"\" href=\"#search-box\">\\n          <svg class=\"icon inline-block vm\"><use xlink:href=\"#icon-search\"></use></svg>\\n      </a>,\n",
       " <a class=\"i-nav_drawer_slide@tp-__close hide@tl+ absolute r-n40 c-gray-2\" href=\"#\">\\n    <svg class=\"icon increase-hit__inner\">\\n      <use xlink:href=\"#icon-close\"></use>\\n    </svg>\\n  </a>,\n",
       " <a class=\"increase-hit@tl+ c-gray-10:hvr block@tp- p-20@tp- o-h@tp- \" href=\"/\">\\n    <div class=\"left@tp-\">\\n      <span class=\"increase-hit@tl+__inner t-global-nav@tp-\">\\n        Home\\n      </span>\\n    </div>\\n      </a>,\n",
       " <a class=\"increase-hit@tl+ c-gray-10:hvr block@tp- p-20@tp- o-h@tp- \" href=\"/gear/\">\\n    <div class=\"left@tp-\">\\n      <span class=\"increase-hit@tl+__inner t-global-nav@tp-\">\\n        Gear\\n      </span>\\n    </div>\\n      </a>,\n",
       " <a class=\"increase-hit@tl+ c-gray-10:hvr block@tp- p-20@tp- o-h@tp- \" href=\"/gaming/\">\\n    <div class=\"left@tp-\">\\n      <span class=\"increase-hit@tl+__inner t-global-nav@tp-\">\\n        Gaming\\n      </span>\\n    </div>\\n      </a>,\n",
       " <a class=\"increase-hit@tl+ c-gray-10:hvr block@tp- p-20@tp- o-h@tp- \" href=\"/culture/\">\\n    <div class=\"left@tp-\">\\n      <span class=\"increase-hit@tl+__inner t-global-nav@tp-\">\\n        Culture\\n      </span>\\n    </div>\\n      </a>,\n",
       " <a class=\"increase-hit@tl+ c-gray-10:hvr block@tp- p-20@tp- o-h@tp- \" href=\"/entertainment/\">\\n    <div class=\"left@tp-\">\\n      <span class=\"increase-hit@tl+__inner t-global-nav@tp-\">\\n        Entertainment\\n      </span>\\n    </div>\\n      </a>,\n",
       " <a class=\"increase-hit@tl+ c-gray-10:hvr block@tp- p-20@tp- o-h@tp- \" href=\"/science/\">\\n    <div class=\"left@tp-\">\\n      <span class=\"increase-hit@tl+__inner t-global-nav@tp-\">\\n        Science\\n      </span>\\n    </div>\\n      </a>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = urllib2.urlopen('http://www.downloadsquad.com/rss.xml')\n",
    "soup = BeautifulSoup(c.read(),\"html5lib\")\n",
    "link = soup('a')\n",
    "link[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "searcher class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class searcher:\n",
    "    def __init__(self,dbname):\n",
    "        self.con = sqlite.connect(dbname)\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.con.close()\n",
    "        \n",
    "    def getmatchrows(self,q):\n",
    "        #构造查询的字符串:\n",
    "        fieldlist  = 'w0.urlid'\n",
    "        tablelist  = ''\n",
    "        clauselist = ''\n",
    "        wordids = []\n",
    "        \n",
    "        #根据空格拆分单词\n",
    "        words = q.split(' ')\n",
    "        tablenumber = 0\n",
    "        \n",
    "        for word in words:\n",
    "            #获取单词id\n",
    "            wordrow = self.con.excute(\"select rowid from wordlist where word = '%s'\" % word).fetchone()\n",
    "            if wordrow != None:\n",
    "                wordid = wordrow[0] #wordrow 是一个元祖\n",
    "                wordids.append(wordid)\n",
    "                if tablenumber >0:\n",
    "                    tablelist += ','\n",
    "                    clauselist += ' and '\n",
    "                    clauselist += 'w%d.urlid = w%d.urlid and '%(tablenumber-1,tablenumber)\n",
    "                fieldlist += ',w%d.location'% tablenumber\n",
    "                tablelist += 'wordlocation'% tablenumber\n",
    "                clauselist += 'w%d.wordid = %d' % (tablenumber,wordid)\n",
    "                tablenumber += 1\n",
    "            \n",
    "            #根据各个组分,建立查询\n",
    "            fullquery = 'select %s from %s where %s'%(fieldlist,tablelist,clauselist)\n",
    "            cur = self.con.excute(fullquery)\n",
    "            rows = [row for row in cur]\n",
    "            \n",
    "            return rows,wordids\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = searcher('seacheindex.db')\n",
    "e.getmatchchrows('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
